Waiting for master pod...
My IP: 10.0.5.181
Waiting... (1/60)
Waiting... (2/60)
Waiting... (3/60)
Waiting... (4/60)
Waiting... (5/60)
Found master: 10.0.4.147
=== Environment ===
WORLD_SIZE=2
RANK=1
MASTER_ADDR=10.0.4.147
MASTER_PORT=29500
MY_IP=10.0.5.181

Waiting for master port...
Master is ready!
[Rank 1/2] Running on GPU 0
[Rank 1] GPU: NVIDIA GB10
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO ENV/Plugin: Could not find: libnccl-env.so
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO cudaDriverVersion 13010
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Bootstrap: Using eth0:10.0.5.181<0>
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NCCL version 2.28.9+cuda13.0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Comm config Blocking set to 1
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v11 (v11)
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v11)
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Successfully loaded external network plugin /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.5.181<0>
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Initialized NET plugin Socket
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Assigned NET plugin Socket to comm
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Using network Socket
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO ncclCommInitRankConfig comm 0xceb0a90 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId f01000 commId 0x764ded0ac00f6160 - Init START
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO RAS client listening socket at ::1<28028>
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Bootstrap timings total 0.002415 (create 0.000025, send 0.000916, recv 0.000541, ring 0.000460, delay 0.000000)
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO ncclTopoGetCpuAffinity: Affinity for GPU 0 is empty, ignoring. (GPU affinity =  ; CPU affinity = 0-19).
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO comm 0xceb0a90 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO P2P Chunksize set to 131072
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0 isAllCudaP2p 1
pytorch-distributed-sample-trainer-0-1:153:180 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
pytorch-distributed-sample-trainer-0-1:153:181 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO ncclCommInitRankConfig comm 0xceb0a90 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId f01000 commId 0x764ded0ac00f6160 - Init COMPLETE
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.14 (kernels 0.13, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
pytorch-distributed-sample-trainer-0-1:153:182 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 2
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/Socket/0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/Socket/0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/Socket/0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/Socket/0
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
  0%|          | 0.00/26.4M [00:00<?, ?B/s]  0%|          | 32.8k/26.4M [00:00<03:32, 124kB/s]  0%|          | 65.5k/26.4M [00:00<03:38, 121kB/s]  0%|          | 98.3k/26.4M [00:00<03:39, 120kB/s]  1%|          | 229k/26.4M [00:01<01:40, 261kB/s]   2%|▏         | 459k/26.4M [00:01<00:55, 467kB/s]  3%|▎         | 918k/26.4M [00:01<00:29, 875kB/s]  7%|▋         | 1.80M/26.4M [00:01<00:15, 1.63MB/s] 10%|▉         | 2.52M/26.4M [00:02<00:16, 1.45MB/s] 11%|█         | 2.92M/26.4M [00:02<00:15, 1.47MB/s] 24%|██▍       | 6.42M/26.4M [00:03<00:04, 4.49MB/s] 29%|██▉       | 7.73M/26.4M [00:03<00:04, 4.49MB/s] 31%|███       | 8.22M/26.4M [00:03<00:04, 3.78MB/s] 40%|████      | 10.6M/26.4M [00:03<00:03, 5.24MB/s] 45%|████▌     | 12.0M/26.4M [00:04<00:02, 5.09MB/s] 50%|█████     | 13.3M/26.4M [00:04<00:02, 4.99MB/s] 55%|█████▌    | 14.6M/26.4M [00:04<00:02, 4.95MB/s] 61%|██████    | 16.0M/26.4M [00:04<00:02, 4.96MB/s] 66%|██████▌   | 17.4M/26.4M [00:05<00:01, 4.97MB/s] 71%|███████   | 18.8M/26.4M [00:05<00:01, 5.01MB/s] 77%|███████▋  | 20.2M/26.4M [00:05<00:01, 5.07MB/s] 82%|████████▏ | 21.7M/26.4M [00:06<00:00, 5.12MB/s] 87%|████████▋ | 23.1M/26.4M [00:06<00:00, 5.15MB/s] 93%|█████████▎| 24.6M/26.4M [00:06<00:00, 5.21MB/s] 99%|█████████▊| 26.1M/26.4M [00:06<00:00, 5.25MB/s]100%|██████████| 26.4M/26.4M [00:06<00:00, 3.83MB/s]
  0%|          | 0.00/29.5k [00:00<?, ?B/s]100%|██████████| 29.5k/29.5k [00:00<00:00, 107kB/s]100%|██████████| 29.5k/29.5k [00:00<00:00, 107kB/s]
  0%|          | 0.00/4.42M [00:00<?, ?B/s]  1%|          | 32.8k/4.42M [00:00<00:37, 117kB/s]  1%|▏         | 65.5k/4.42M [00:00<00:37, 117kB/s]  2%|▏         | 98.3k/4.42M [00:00<00:36, 117kB/s]  5%|▌         | 229k/4.42M [00:01<00:16, 256kB/s]  10%|█         | 459k/4.42M [00:01<00:08, 459kB/s] 21%|██        | 918k/4.42M [00:01<00:04, 859kB/s] 41%|████      | 1.80M/4.42M [00:01<00:01, 1.61MB/s] 69%|██████▉   | 3.05M/4.42M [00:02<00:00, 2.48MB/s] 76%|███████▋  | 3.38M/4.42M [00:02<00:00, 2.10MB/s]100%|██████████| 4.42M/4.42M [00:02<00:00, 1.75MB/s]
  0%|          | 0.00/5.15k [00:00<?, ?B/s]100%|██████████| 5.15k/5.15k [00:00<00:00, 44.1MB/s]
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO comm 0xceb0a90 rank 1 nranks 2 cudaDev 0 busId f01000 - Destroy COMPLETE
pytorch-distributed-sample-trainer-0-1:153:153 [0] NCCL INFO ENV/Plugin: Closing env plugin ncclEnvDefault

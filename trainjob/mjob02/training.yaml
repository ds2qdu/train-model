---
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: pytorch-distributed-sample
  namespace: mlteam
  annotations:
    kai.scheduler/queue: "mlteam-queue"
spec:
  runtimeRef:
    name: pytorch-distributed
    kind: TrainingRuntime
    apiGroup: trainer.kubeflow.org 
  trainer:
    image: nvcr.io/nvidia/pytorch:25.01-py3
    command:
      - torchrun
    args:
      - --nnodes=2
      - --nproc_per_node=1
      - --master_addr=$(MASTER_ADDR)
      - --master_port=$(MASTER_PORT)
      - /workspace/train.py
    numNodes: 2
    numProcPerNode: auto
    resourcesPerNode:
      requests:
        cpu: "4"
        memory: "16Gi"
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: "32Gi"
        nvidia.com/gpu: "1"
    env:
      - name: NCCL_DEBUG
        value: "INFO"
      - name: NCCL_IB_DISABLE
        value: "1"
  podTemplateOverrides:
    - targetJobs:
        - name: node
      spec:
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"


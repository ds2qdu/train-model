=== Environment ===
WORLD_SIZE=2
RANK=0
MASTER_ADDR=10.0.4.147
MASTER_PORT=29500
MY_IP=10.0.4.147

[Rank 0/2] Running on GPU 0
[Rank 0] GPU: NVIDIA GB10
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO ENV/Plugin: Could not find: libnccl-env.so
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Bootstrap: Using eth0:10.0.4.147<0>
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO cudaDriverVersion 13010
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NCCL version 2.28.9+cuda13.0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Comm config Blocking set to 1
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v11 (v11)
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NET/Plugin: Loaded collnet plugin SHARP (v11)
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Successfully loaded external network plugin /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.4.147<0>
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Initialized NET plugin Socket
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Assigned NET plugin Socket to comm
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Using network Socket
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO ncclCommInitRankConfig comm 0x499c6d40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId f01000 commId 0x764ded0ac00f6160 - Init START
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO RAS client listening socket at ::1<28028>
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Bootstrap timings total 0.059086 (create 0.000023, send 0.000078, recv 0.057811, ring 0.000087, delay 0.000000)
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO ncclTopoGetCpuAffinity: Affinity for GPU 0 is empty, ignoring. (GPU affinity =  ; CPU affinity = 0-19).
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO comm 0x499c6d40 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 00/02 : 0 1
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 01/02 : 0 1
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO P2P Chunksize set to 131072
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0 isAllCudaP2p 1
pytorch-distributed-sample-trainer-0-0:105:134 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
pytorch-distributed-sample-trainer-0-0:105:133 [0] NCCL INFO [Proxy Service] Device 0 CPU core 16
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO CC Off, workFifoBytes 1048576
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO ncclCommInitRankConfig comm 0x499c6d40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId f01000 commId 0x764ded0ac00f6160 - Init COMPLETE
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.20 (kernels 0.13, alloc 0.00, bootstrap 0.06, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
pytorch-distributed-sample-trainer-0-0:105:135 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/Socket/0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/Socket/0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/Socket/0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/Socket/0
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
  0%|          | 0.00/26.4M [00:00<?, ?B/s]  0%|          | 32.8k/26.4M [00:00<03:36, 122kB/s]  0%|          | 65.5k/26.4M [00:00<03:40, 119kB/s]  0%|          | 98.3k/26.4M [00:00<03:41, 119kB/s]  1%|          | 229k/26.4M [00:01<01:41, 258kB/s]   2%|▏         | 459k/26.4M [00:01<00:56, 461kB/s]  3%|▎         | 918k/26.4M [00:01<00:29, 865kB/s]  7%|▋         | 1.80M/26.4M [00:01<00:15, 1.61MB/s]  8%|▊         | 2.03M/26.4M [00:02<00:18, 1.35MB/s] 10%|█         | 2.65M/26.4M [00:02<00:14, 1.64MB/s] 17%|█▋        | 4.42M/26.4M [00:02<00:07, 3.09MB/s] 22%|██▏       | 5.83M/26.4M [00:03<00:05, 3.69MB/s] 28%|██▊       | 7.34M/26.4M [00:03<00:04, 4.22MB/s] 34%|███▎      | 8.88M/26.4M [00:03<00:03, 4.61MB/s] 40%|███▉      | 10.5M/26.4M [00:03<00:03, 4.92MB/s] 46%|████▌     | 12.1M/26.4M [00:04<00:02, 5.17MB/s] 52%|█████▏    | 13.7M/26.4M [00:04<00:01, 6.46MB/s] 55%|█████▍    | 14.5M/26.4M [00:04<00:01, 6.04MB/s] 58%|█████▊    | 15.4M/26.4M [00:04<00:02, 5.14MB/s] 65%|██████▍   | 17.1M/26.4M [00:04<00:01, 6.76MB/s] 68%|██████▊   | 17.9M/26.4M [00:05<00:01, 6.25MB/s] 71%|███████▏  | 18.9M/26.4M [00:05<00:01, 5.26MB/s] 78%|███████▊  | 20.6M/26.4M [00:05<00:01, 5.66MB/s] 85%|████████▍ | 22.4M/26.4M [00:05<00:00, 5.93MB/s] 92%|█████████▏| 24.3M/26.4M [00:06<00:00, 6.15MB/s] 99%|█████████▉| 26.1M/26.4M [00:06<00:00, 7.65MB/s]100%|██████████| 26.4M/26.4M [00:06<00:00, 4.12MB/s]
  0%|          | 0.00/29.5k [00:00<?, ?B/s]100%|██████████| 29.5k/29.5k [00:00<00:00, 107kB/s]100%|██████████| 29.5k/29.5k [00:00<00:00, 107kB/s]
  0%|          | 0.00/4.42M [00:00<?, ?B/s]  1%|          | 32.8k/4.42M [00:00<00:37, 118kB/s]  1%|▏         | 65.5k/4.42M [00:00<00:37, 117kB/s]  2%|▏         | 98.3k/4.42M [00:00<00:36, 117kB/s]  5%|▌         | 229k/4.42M [00:01<00:16, 256kB/s]  10%|█         | 459k/4.42M [00:01<00:08, 459kB/s] 21%|██        | 918k/4.42M [00:01<00:04, 861kB/s] 41%|████      | 1.80M/4.42M [00:01<00:01, 1.61MB/s] 74%|███████▍  | 3.28M/4.42M [00:02<00:00, 2.73MB/s] 81%|████████  | 3.57M/4.42M [00:02<00:00, 2.25MB/s]100%|██████████| 4.42M/4.42M [00:02<00:00, 1.75MB/s]
  0%|          | 0.00/5.15k [00:00<?, ?B/s]100%|██████████| 5.15k/5.15k [00:00<00:00, 28.4MB/s]
Epoch [1/5] Batch [0/469] Loss: 2.3110
Epoch [1/5] Batch [100/469] Loss: 0.6041
Epoch [1/5] Batch [200/469] Loss: 0.4313
Epoch [1/5] Batch [300/469] Loss: 0.7241
Epoch [1/5] Batch [400/469] Loss: 0.3515
Epoch [1/5] Average Loss: 0.5048
Epoch [2/5] Batch [0/469] Loss: 0.2921
Epoch [2/5] Batch [100/469] Loss: 0.3681
Epoch [2/5] Batch [200/469] Loss: 0.3793
Epoch [2/5] Batch [300/469] Loss: 0.5352
Epoch [2/5] Batch [400/469] Loss: 0.2304
Epoch [2/5] Average Loss: 0.3679
Epoch [3/5] Batch [0/469] Loss: 0.3971
Epoch [3/5] Batch [100/469] Loss: 0.3562
Epoch [3/5] Batch [200/469] Loss: 0.2893
Epoch [3/5] Batch [300/469] Loss: 0.3183
Epoch [3/5] Batch [400/469] Loss: 0.3789
Epoch [3/5] Average Loss: 0.3240
Epoch [4/5] Batch [0/469] Loss: 0.3116
Epoch [4/5] Batch [100/469] Loss: 0.3334
Epoch [4/5] Batch [200/469] Loss: 0.3931
Epoch [4/5] Batch [300/469] Loss: 0.1742
Epoch [4/5] Batch [400/469] Loss: 0.1417
Epoch [4/5] Average Loss: 0.2966
Epoch [5/5] Batch [0/469] Loss: 0.1667
Epoch [5/5] Batch [100/469] Loss: 0.3119
Epoch [5/5] Batch [200/469] Loss: 0.2667
Epoch [5/5] Batch [300/469] Loss: 0.2543
Epoch [5/5] Batch [400/469] Loss: 0.3247
Epoch [5/5] Average Loss: 0.2802
Training completed!
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO comm 0x499c6d40 rank 0 nranks 2 cudaDev 0 busId f01000 - Destroy COMPLETE
pytorch-distributed-sample-trainer-0-0:105:105 [0] NCCL INFO ENV/Plugin: Closing env plugin ncclEnvDefault

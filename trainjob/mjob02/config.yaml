---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pytorch-train-script
  namespace: ml-team-a
data:
  train.py: |
    import os
    import torch
    import torch.distributed as dist
    import torch.nn as nn
    import torch.optim as optim
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data import DataLoader, DistributedSampler
    from torchvision import datasets, transforms

    def setup():
        dist.init_process_group(backend="nccl")
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        return local_rank

    def cleanup():
        dist.destroy_process_group()

    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(784, 512)
            self.fc2 = nn.Linear(512, 256)
            self.fc3 = nn.Linear(256, 10)
            self.relu = nn.ReLU()

        def forward(self, x):
            x = x.view(-1, 784)
            x = self.relu(self.fc1(x))
            x = self.relu(self.fc2(x))
            return self.fc3(x)

    def main():
        local_rank = setup()
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        
        print(f"[Rank {rank}/{world_size}] Running on GPU {local_rank}")
        print(f"[Rank {rank}] GPU: {torch.cuda.get_device_name(local_rank)}")

        model = SimpleModel().cuda(local_rank)
        model = DDP(model, device_ids=[local_rank])

        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])
        
        dataset = datasets.FashionMNIST(
            root='/tmp/data',
            train=True,
            download=True,
            transform=transform
        )
        
        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
        dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        num_epochs = 5
        for epoch in range(num_epochs):
            sampler.set_epoch(epoch)
            model.train()
            total_loss = 0.0
            
            for batch_idx, (data, target) in enumerate(dataloader):
                data, target = data.cuda(local_rank), target.cuda(local_rank)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 100 == 0 and rank == 0:
                    print(f"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx}/{len(dataloader)}] Loss: {loss.item():.4f}")
            
            avg_loss = total_loss / len(dataloader)
            if rank == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}")

        if rank == 0:
            print("Training completed!")
            torch.save(model.module.state_dict(), '/tmp/model.pt')

        cleanup()

    if __name__ == "__main__":
        main()


# 02-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mnist-train-script
  namespace: mlteam
data:
  train.py: |
    import os
    import argparse
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data import DataLoader, DistributedSampler
    from torchvision import datasets, transforms
    from pathlib import Path
    import json
    from datetime import datetime

    def setup():
        dist.init_process_group(backend="nccl")
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        return local_rank

    def cleanup():
        dist.destroy_process_group()

    class MNISTNet(nn.Module):
        """간단한 CNN 모델"""
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
            self.pool = nn.MaxPool2d(2, 2)
            self.dropout1 = nn.Dropout(0.25)
            self.dropout2 = nn.Dropout(0.5)
            self.fc1 = nn.Linear(128 * 3 * 3, 256)
            self.fc2 = nn.Linear(256, 10)
            self.relu = nn.ReLU()

        def forward(self, x):
            # 입력: (batch, 1, 28, 28)
            x = self.pool(self.relu(self.conv1(x)))  # (batch, 32, 14, 14)
            x = self.pool(self.relu(self.conv2(x)))  # (batch, 64, 7, 7)
            x = self.pool(self.relu(self.conv3(x)))  # (batch, 128, 3, 3)
            x = self.dropout1(x)
            x = x.view(-1, 128 * 3 * 3)
            x = self.relu(self.fc1(x))
            x = self.dropout2(x)
            x = self.fc2(x)
            return x

    def save_checkpoint(model, optimizer, epoch, loss, accuracy, checkpoint_dir, is_best=False):
        """체크포인트 저장"""
        checkpoint_dir = Path(checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.module.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'accuracy': accuracy,
            'timestamp': datetime.now().isoformat()
        }
        
        # 최신 체크포인트
        torch.save(checkpoint, checkpoint_dir / 'checkpoint_latest.pt')
        print(f"Checkpoint saved: epoch {epoch+1}, accuracy {accuracy:.2f}%")
        
        # 최고 성능 모델
        if is_best:
            torch.save(checkpoint, checkpoint_dir / 'checkpoint_best.pt')
            print(f"Best model saved: {accuracy:.2f}%")

    def load_checkpoint(model, optimizer, checkpoint_dir):
        """체크포인트 로드"""
        checkpoint_path = Path(checkpoint_dir) / 'checkpoint_latest.pt'
        
        if checkpoint_path.exists():
            print(f"Loading checkpoint: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location='cuda')
            model.module.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            return checkpoint['epoch'] + 1, checkpoint.get('accuracy', 0)
        return 0, 0

    def evaluate(model, dataloader, criterion, device):
        """모델 평가"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        accuracy = 100. * correct / total
        avg_loss = total_loss / len(dataloader)
        return avg_loss, accuracy

    def export_model(model, export_dir):
        """모델 Export"""
        export_dir = Path(export_dir)
        export_dir.mkdir(parents=True, exist_ok=True)
        
        model.eval()
        device = next(model.parameters()).device
        dummy_input = torch.randn(1, 1, 28, 28).to(device)
        
        # TorchScript
        scripted = torch.jit.trace(model, dummy_input)
        scripted.save(str(export_dir / 'model.pt'))
        print(f"TorchScript saved: {export_dir / 'model.pt'}")
        
        # ONNX
        torch.onnx.export(
            model, dummy_input,
            str(export_dir / 'model.onnx'),
            export_params=True,
            opset_version=17,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}
        )
        print(f"ONNX saved: {export_dir / 'model.onnx'}")
        
        # 메타데이터
        metadata = {
            'model_name': 'mnist-cnn',
            'task': 'digit_classification',
            'num_classes': 10,
            'input_shape': [1, 1, 28, 28],
            'class_names': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
            'preprocessing': {
                'resize': [28, 28],
                'normalize_mean': [0.1307],
                'normalize_std': [0.3081],
                'grayscale': True
            },
            'export_timestamp': datetime.now().isoformat()
        }
        with open(export_dir / 'metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"Metadata saved: {export_dir / 'metadata.json'}")

    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument('--epochs', type=int, default=10)
        parser.add_argument('--batch-size', type=int, default=128)
        parser.add_argument('--lr', type=float, default=0.001)
        parser.add_argument('--data-dir', type=str, default='/tmp/data')
        parser.add_argument('--checkpoint-dir', type=str, default='/mnt/storage/checkpoints')
        parser.add_argument('--export-dir', type=str, default='/mnt/storage/models')
        parser.add_argument('--resume', action='store_true')
        args = parser.parse_args()

        local_rank = setup()
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        device = torch.device(f'cuda:{local_rank}')
        
        if rank == 0:
            print("=" * 50)
            print("MNIST Digit Recognition Training")
            print("=" * 50)
            print(f"World size: {world_size}")
            print(f"Arguments: {args}")

        # 데이터 준비
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        train_dataset = datasets.MNIST(args.data_dir, train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST(args.data_dir, train=False, download=True, transform=transform)
        
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler, num_workers=4)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4) if rank == 0 else None

        # 모델
        model = MNISTNet().to(device)
        model = DDP(model, device_ids=[local_rank])

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

        # 체크포인트 로드
        start_epoch, best_accuracy = 0, 0.0
        if args.resume:
            start_epoch, best_accuracy = load_checkpoint(model, optimizer, args.checkpoint_dir)

        # 학습
        for epoch in range(start_epoch, args.epochs):
            train_sampler.set_epoch(epoch)
            model.train()
            total_loss = 0.0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 100 == 0 and rank == 0:
                    print(f"Epoch [{epoch+1}/{args.epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}")
            
            scheduler.step()
            
            # 평가 및 저장
            if rank == 0:
                test_loss, accuracy = evaluate(model.module, test_loader, criterion, device)
                print(f"Epoch [{epoch+1}/{args.epochs}] Test Loss: {test_loss:.4f} Accuracy: {accuracy:.2f}%")
                
                is_best = accuracy > best_accuracy
                if is_best:
                    best_accuracy = accuracy
                save_checkpoint(model, optimizer, epoch, test_loss, accuracy, args.checkpoint_dir, is_best)
            
            dist.barrier()

        # Export
        if rank == 0:
            print("\n" + "=" * 50)
            print("Exporting model...")
            print("=" * 50)
            
            best_ckpt = Path(args.checkpoint_dir) / 'checkpoint_best.pt'
            if best_ckpt.exists():
                ckpt = torch.load(best_ckpt)
                model.module.load_state_dict(ckpt['model_state_dict'])
                print(f"Loaded best model: {ckpt['accuracy']:.2f}%")
            
            export_model(model.module, args.export_dir)
            
            # 결과 요약
            summary = {
                'task': 'MNIST Digit Recognition',
                'best_accuracy': best_accuracy,
                'total_epochs': args.epochs,
                'world_size': world_size
            }
            with open(Path(args.export_dir) / 'training_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"\n✅ Training completed! Best accuracy: {best_accuracy:.2f}%")

        cleanup()

    if __name__ == "__main__":
        main()

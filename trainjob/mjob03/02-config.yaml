apiVersion: v1
kind: ConfigMap
metadata:
  name: mnist-train-script
  namespace: ml-team-a
data:
  train.py: |
    import os
    import argparse
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.utils.data import DataLoader, DistributedSampler
    from torchvision import datasets, transforms
    from pathlib import Path
    import json
    from datetime import datetime

    def setup():
        dist.init_process_group(backend="nccl")
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        return local_rank

    def cleanup():
        dist.destroy_process_group()

    class MNISTNet(nn.Module):
        """CNN 모델"""
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
            self.pool = nn.MaxPool2d(2, 2)
            self.dropout1 = nn.Dropout(0.25)
            self.dropout2 = nn.Dropout(0.5)
            self.fc1 = nn.Linear(128 * 3 * 3, 256)
            self.fc2 = nn.Linear(256, 10)
            self.relu = nn.ReLU()

        def forward(self, x):
            x = self.pool(self.relu(self.conv1(x)))
            x = self.pool(self.relu(self.conv2(x)))
            x = self.pool(self.relu(self.conv3(x)))
            x = self.dropout1(x)
            x = x.view(-1, 128 * 3 * 3)
            x = self.relu(self.fc1(x))
            x = self.dropout2(x)
            x = self.fc2(x)
            return x

    def save_checkpoint(model, optimizer, epoch, loss, accuracy, checkpoint_dir, is_best=False):
        checkpoint_dir = Path(checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.module.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'accuracy': accuracy,
            'timestamp': datetime.now().isoformat()
        }
        
        torch.save(checkpoint, checkpoint_dir / 'checkpoint_latest.pt')
        print(f"Checkpoint saved: epoch {epoch+1}, accuracy {accuracy:.2f}%")
        
        if is_best:
            torch.save(checkpoint, checkpoint_dir / 'checkpoint_best.pt')
            print(f"Best model saved: {accuracy:.2f}%")

    def load_checkpoint(model, optimizer, checkpoint_dir):
        checkpoint_path = Path(checkpoint_dir) / 'checkpoint_latest.pt'
        
        if checkpoint_path.exists():
            print(f"Loading checkpoint: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location='cuda')
            model.module.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            return checkpoint['epoch'] + 1, checkpoint.get('accuracy', 0)
        return 0, 0

    def evaluate(model, dataloader, criterion, device):
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        return total_loss / len(dataloader), 100. * correct / total

    def export_model_for_triton(model, export_dir):
        """Triton 형식으로 모델 Export"""
        export_dir = Path(export_dir)
        
        # Triton 디렉토리 구조: mnist/1/model.onnx
        triton_model_dir = export_dir / "mnist" / "1"
        triton_model_dir.mkdir(parents=True, exist_ok=True)
        
        model.eval()
        device = next(model.parameters()).device
        dummy_input = torch.randn(1, 1, 28, 28).to(device)
        
        # ONNX 저장
        onnx_path = triton_model_dir / 'model.onnx'
        torch.onnx.export(
            model, dummy_input,
            str(onnx_path),
            export_params=True,
            opset_version=17,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}
        )
        print(f"ONNX model saved: {onnx_path}")
        
        # Triton config.pbtxt 생성
        config_content = """name: "mnist"
    platform: "onnxruntime_onnx"
    max_batch_size: 64
    input [
      {
        name: "input"
        data_type: TYPE_FP32
        dims: [ 1, 28, 28 ]
      }
    ]
    output [
      {
        name: "output"
        data_type: TYPE_FP32
        dims: [ 10 ]
      }
    ]
    instance_group [
      {
        count: 1
        kind: KIND_GPU
      }
    ]
    dynamic_batching {
      preferred_batch_size: [ 8, 16, 32 ]
      max_queue_delay_microseconds: 100
    }
    """
        config_path = export_dir / "mnist" / "config.pbtxt"
        with open(config_path, 'w') as f:
            f.write(config_content)
        print(f"Triton config saved: {config_path}")
        
        # 메타데이터 저장
        metadata = {
            'model_name': 'mnist',
            'task': 'digit_classification',
            'num_classes': 10,
            'input_shape': [1, 1, 28, 28],
            'class_names': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
            'preprocessing': {
                'normalize_mean': 0.1307,
                'normalize_std': 0.3081
            },
            'export_timestamp': datetime.now().isoformat()
        }
        with open(export_dir / 'metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"Metadata saved: {export_dir / 'metadata.json'}")

    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument('--epochs', type=int, default=10)
        parser.add_argument('--batch-size', type=int, default=128)
        parser.add_argument('--lr', type=float, default=0.001)
        parser.add_argument('--data-dir', type=str, default='/tmp/data')
        parser.add_argument('--checkpoint-dir', type=str, default='/mnt/storage/checkpoints')
        parser.add_argument('--export-dir', type=str, default='/mnt/storage/models')
        parser.add_argument('--resume', action='store_true')
        args = parser.parse_args()

        local_rank = setup()
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        device = torch.device(f'cuda:{local_rank}')
        
        if rank == 0:
            print("=" * 50)
            print("MNIST Digit Recognition Training")
            print("=" * 50)
            print(f"World size: {world_size}")
            print(f"Arguments: {args}")

        # 데이터 준비
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        train_dataset = datasets.MNIST(args.data_dir, train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST(args.data_dir, train=False, download=True, transform=transform)
        
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler, num_workers=4)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4) if rank == 0 else None

        # 모델
        model = MNISTNet().to(device)
        model = DDP(model, device_ids=[local_rank])

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

        # 체크포인트 로드
        start_epoch, best_accuracy = 0, 0.0
        if args.resume:
            start_epoch, best_accuracy = load_checkpoint(model, optimizer, args.checkpoint_dir)

        # 학습
        for epoch in range(start_epoch, args.epochs):
            train_sampler.set_epoch(epoch)
            model.train()
            total_loss = 0.0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 100 == 0 and rank == 0:
                    print(f"Epoch [{epoch+1}/{args.epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}")
            
            scheduler.step()
            
            # 평가 및 저장
            if rank == 0:
                test_loss, accuracy = evaluate(model.module, test_loader, criterion, device)
                print(f"Epoch [{epoch+1}/{args.epochs}] Test Loss: {test_loss:.4f} Accuracy: {accuracy:.2f}%")
                
                is_best = accuracy > best_accuracy
                if is_best:
                    best_accuracy = accuracy
                save_checkpoint(model, optimizer, epoch, test_loss, accuracy, args.checkpoint_dir, is_best)
            
            dist.barrier()

        # Export
        if rank == 0:
            print("\n" + "=" * 50)
            print("Exporting model for Triton...")
            print("=" * 50)
            
            best_ckpt = Path(args.checkpoint_dir) / 'checkpoint_best.pt'
            if best_ckpt.exists():
                ckpt = torch.load(best_ckpt)
                model.module.load_state_dict(ckpt['model_state_dict'])
                print(f"Loaded best model: {ckpt['accuracy']:.2f}%")
            
            export_model_for_triton(model.module, args.export_dir)
            
            # 결과 요약
            summary = {
                'task': 'MNIST Digit Recognition',
                'best_accuracy': best_accuracy,
                'total_epochs': args.epochs,
                'world_size': world_size,
                'export_format': 'ONNX (Triton)',
                'completed_at': datetime.now().isoformat()
            }
            with open(Path(args.export_dir) / 'training_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"\n✅ Training completed! Best accuracy: {best_accuracy:.2f}%")
            print(f"✅ Model exported for Triton at: {args.export_dir}/mnist/")

        cleanup()

    if __name__ == "__main__":
        main()

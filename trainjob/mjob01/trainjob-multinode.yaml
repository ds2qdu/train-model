apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: trainjob-multi-node-ddp
  namespace: jhchoi
spec:
  managedBy: trainer.kubeflow.org/trainjob-controller
  runtimeRef:
    apiGroup: trainer.kubeflow.org
    kind: TrainingRuntime
    name: multi-node-torch-distributed
  suspend: false
  trainer:
    numNodes: 2
    image: nvcr.io/nvidia/pytorch:25.01-py3
    command:
      - "bash"
      - "-c"
      - |
        # Pod 이름에서 환경변수 추출 (예: trainjob-multi-node-ddp-node-0-0-xxxxx)
        # JOB_COMPLETION_INDEX는 JobSet이 자동 설정
        # WORD_SIZE: 총 노드수 
        # RANK: JobSet 이 자동 할당 (0,1)
        # MASTER_ADDR: Pod 0 의 DNS, Headless Service 기반
        # MASTER_PORT: 29500, 기본 torchrun port
        export WORLD_SIZE=2
        export RANK=${JOB_COMPLETION_INDEX:-0}
        export MASTER_ADDR="trainjob-multi-node-ddp-node-0-0.jhchoi.svc.cluster.local"
        export MASTER_PORT=29500
        export LOCAL_RANK=0

        echo "=== Configured Environment ==="
        echo "WORLD_SIZE=$WORLD_SIZE"
        echo "RANK=$RANK"
        echo "MASTER_ADDR=$MASTER_ADDR"
        echo "MASTER_PORT=$MASTER_PORT"
        echo ""

        cat > /tmp/ddp_train.py << 'EOF'
        import os
        import torch
        import torch.distributed as dist
        import torch.nn as nn
        from torch.nn.parallel import DistributedDataParallel as DDP

        def main():
            # 분산 환경 초기화
            dist.init_process_group(backend="nccl")
            rank = dist.get_rank()
            world_size = dist.get_world_size()
            local_rank = int(os.environ.get("LOCAL_RANK", 0))

            print(f"[Rank {rank}/{world_size}] Running on {torch.cuda.get_device_name(local_rank)}")
            print(f"[Rank {rank}] MASTER_ADDR={os.environ.get('MASTER_ADDR')}")
            print(f"[Rank {rank}] MASTER_PORT={os.environ.get('MASTER_PORT')}")

            # 모델 생성 및 DDP 래핑
            torch.cuda.set_device(local_rank)
            model = nn.Linear(1000, 1000).cuda(local_rank)
            model = DDP(model, device_ids=[local_rank])

            # 10분간 지속적인 학습 테스트
            import time
            duration_minutes = 10

            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

            # 큰 배치로 GPU 부하 증가
            x = torch.randn(256, 1000).cuda(local_rank)

            start_time = time.time()
            iteration = 0

            if rank == 0:
                print(f"Starting {duration_minutes} minute distributed training test...")

            while (time.time() - start_time) < (duration_minutes * 60):
                y = model(x)
                loss = y.sum()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                iteration += 1

                if iteration % 500 == 0 and rank == 0:
                    elapsed = time.time() - start_time
                    print(f"  Iteration {iteration}, Elapsed: {elapsed:.1f}s, Loss: {loss.item():.4f}")

            total_time = time.time() - start_time
            if rank == 0:
                print(f"\nCompleted {iteration} iterations in {total_time:.2f} seconds")

            dist.destroy_process_group()
            print(f"[Rank {rank}] Training completed!")

        if __name__ == "__main__":
            main()
        EOF

        torchrun \
          --nnodes=$WORLD_SIZE \
          --node_rank=$RANK \
          --master_addr=$MASTER_ADDR \
          --master_port=$MASTER_PORT \
          --nproc_per_node=1 \
          /tmp/ddp_train.py
    env:
      - name: NCCL_DEBUG
        value: "INFO"
      - name: NCCL_SOCKET_FAMILY
        value: "AF_INET"
      - name: GLOO_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
    resourcesPerNode:
      limits:
        cpu: 4
        memory: 8Gi
        nvidia.com/gpu: 1
      requests:
        cpu: 4
        memory: 8Gi
        nvidia.com/gpu: 1
  podTemplateOverrides:
    - targetJobs:
        - name: node
      spec:
        schedulerName: kai-scheduler
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
  annotations:
    # Gang scheduling
    scheduling.x-k8s.io/pod-group-min-available: "2"

# ============================================
# YOLO Object Detection - Distributed Training
# 2 Nodes with GPU for MLOps GPU Validation
# ============================================
apiVersion: kubeflow.org/v2alpha1
kind: TrainJob
metadata:
  name: yolo-distributed-training
  namespace: mlteam
spec:
  suspend: false
  runtimeRef:
    name: torch-distributed-runtime
  trainer:
    image: nvcr.io/nvidia/pytorch:24.01-py3
    command:
      - bash
      - -c
      - |
        # Install dependencies
        echo "=== Installing YOLO and dependencies ==="
        pip install --root-user-action=ignore ultralytics opencv-python-headless albumentations

        # Environment setup for distributed training
        export WORLD_SIZE=2
        export RANK=${JOB_COMPLETION_INDEX:-0}
        export MASTER_PORT=29500
        export LOCAL_RANK=0

        # Get master address
        MASTER_POD_NAME="yolo-distributed-training-trainer-0-0"
        export MASTER_ADDR=$(python -c "import socket; print(socket.gethostbyname('$MASTER_POD_NAME.mlteam.svc.cluster.local'))" 2>/dev/null || echo "$MASTER_POD_NAME")

        echo "=== Distributed Training Config ==="
        echo "WORLD_SIZE: $WORLD_SIZE"
        echo "RANK: $RANK"
        echo "MASTER_ADDR: $MASTER_ADDR"
        echo "MASTER_PORT: $MASTER_PORT"

        # GPU info
        echo "=== GPU Information ==="
        nvidia-smi

        # Run training
        echo "=== Starting YOLO Training ==="
        cd /mnt/storage
        python /mnt/storage/train.py \
          --epochs 100 \
          --batch-size 16 \
          --img-size 640 \
          --data-dir /mnt/storage/data \
          --output-dir /mnt/storage/runs

        echo "=== Training Complete ==="
    resourcesPerNode:
      requests:
        nvidia.com/gpu: "1"
        memory: "16Gi"
        cpu: "4"
      limits:
        nvidia.com/gpu: "1"
        memory: "32Gi"
        cpu: "8"
    numNodes: 2
    env:
      - name: NCCL_DEBUG
        value: "INFO"
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: TOKENIZERS_PARALLELISM
        value: "false"
    volumeMounts:
      - name: storage
        mountPath: /mnt/storage
  podSpecOverrides:
    - targetJobs:
        - name: trainer
      containers:
        - name: trainer
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: yolo-training-storage
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: "16Gi"
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
